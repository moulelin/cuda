<p align="center"><img src="https://github.com/moulelin/Pictures/raw/master/logo_cuda.png" width="950px"> </a>

<p align="center">
  <a href="#"><img src="https://img.shields.io/badge/version-1.0.0-green" alt="Version"></a>
  <a href="#"><img src="https://img.shields.io/badge/downloads-git-g" alt="Download"></a>
  <a href="#"><img src="https://img.shields.io/badge/licence-mit-1" alt="License"></a>
<br>
  <a href="https://github.com/moulelin/cuda/issues"><img src="https://img.shields.io/badge/chat-issue-1)" alt="Chat"></a>
</p>

---

# CUDA-Extension with Pytorch

## Introduction

* [Basic Knowledge of CUDA](https://github.com/moulelin/CUDA-NEFU/wiki/CUDA-Definition)
   
<!-- * [CUDA/C++ with Pytorch](https://github.com/moulelin/CUDA-NEFU/wiki/CUDA-Grammar) -->

* [CUDA/C++ with Pytorch](https://github.com/moulelin/CUDA-NEFU/wiki/CUDA-Pytorch)

<!-- * [Research Recently](https://github.com/moulelin/CUDA-NEFU/wiki/CUDA-Research) 
-->

## Tuturial

* [Basic01 (**Global** function)](https://github.com/moulelin/CUDA-NEFU/wiki/CUDA-Pytorch)
  - [**code here**](https://github.com/moulelin/cuda/tree/master/basic01)
   

* [Basic02 (**share** memory)](https://github.com/moulelin/CUDA-NEFU/wiki/CUDA-Pytorch)
  - [**code here**](https://github.com/moulelin/cuda/tree/master/basic01)

    <!-- * `WIKIPEDIA` Definition: https://en.wikipedia.org/wiki/CUDA*



- Papers used CUDA at conferences CVPR2022
  - [AdaInt](https://github.com/imcharlesy/adaint)

  - [DeepVision3D](https://github.com/dvlab-research/deepvision3d)
  
  - [ShiftGCN](https://github.com/kchengiva/Shift-GCN)
  
  
  - [TwoStageAlign](https://github.com/guoshi28/2stagealign)
  
  - [robustMatch](https://github.com/thinklab-sjtu/robustmatch)
  
  - .............
  
Content of this repository:

- CUDA Definition. 
- CUDA with C++
- CUDA/C++ with Pytorch

## Extension

| Component | Description |
| ---- | --- |
| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |
| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |
| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |
| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |
| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |
| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |



--------------------------------------------------------------------------------
This material is made for presentation purposes for the Group Report of NEFU ICEC -->






